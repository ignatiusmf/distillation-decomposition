# Experiment Beta

Full-scale training run across all datasets and KD methods. Started ~2026-02-12.
Trained models live in `experiments/` at the project root (to be moved here when all runs complete).

## Scope

- **Datasets:** CIFAR-10, CIFAR-100, SVHN, TinyImageNet
- **Methods:** pure (no KD), logit, factor_transfer, attention_transfer, fitnets, rkd, nst
- **Seeds per condition:** varies (see below — `runner.py` had `runs = 1` for most)
- **Teacher:** ResNet-112 (18 blocks/group, channels 16→32→64, GAP→linear)
- **Student:** ResNet-56 (9 blocks/group, same channel structure)
- **Architecture source:** `toolbox/models.py` — `ResNet_simple` with `BasicBlock`, `forward()` returns `[out1, out2, out3, logits]`

## Training Configuration

All experiments use identical hyperparameters (no per-method tuning).

| Parameter | Value | Source |
|-----------|-------|--------|
| Epochs | 150 | `train.py` line 19: `EPOCHS = 150` |
| Batch size | 128 | `train.py` line 20: `BATCH_SIZE = 128` |
| Optimizer | SGD | `train.py` line 140: `optim.SGD(trainable_params, lr=0.1, momentum=0.9, weight_decay=5e-4)` |
| Learning rate | 0.1 | Same line |
| Momentum | 0.9 | Same line |
| Weight decay | 5e-4 | Same line |
| LR schedule | CosineAnnealingLR | `train.py` line 141: `CosineAnnealingLR(optimizer, T_max=EPOCHS)` |
| Label smoothing | 0.1 | `train.py` line 192: `F.cross_entropy(outputs[-1], targets, label_smoothing=0.1)` |
| KD alpha | 0.5 | `runner.py` line 135: `alpha=0.5` — applies to ALL methods |
| KD temperature | 4.0 | `runner.py` line 135: `temperature=4.0` — only used by logit method |
| Loss formula | `(1 - alpha) * CE + alpha * distill_loss` | `train.py` line 200 |
| Device | CUDA | CHPC cluster GPU nodes |

### Seeding (deterministic training)
```python
# train.py lines 102-107
torch.manual_seed(seed)
torch.cuda.manual_seed(seed)
np.random.seed(seed)
random.seed(seed)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False
```

### KD Method Details

All methods receive the same `alpha=0.5`. The distillation loss function differs per method (implementations in `toolbox/distillation.py`):

| Method | What It Matches | Loss Operates On |
|--------|----------------|------------------|
| `logit` | Softened output distributions | KL divergence on logits/temperature |
| `factor_transfer` | Factorised intermediate representations | L2 on paraphraser/translator outputs |
| `attention_transfer` | Spatial attention maps | L2 on attention maps (sum of squared activations over channels) |
| `fitnets` | Raw intermediate features | L2 on hint/guided layer pair (with regressor) |
| `rkd` | Inter-sample distance and angle relations | Distance + angle loss on batch pairs |
| `nst` | Neuron selectivity patterns | MMD-style loss on activation distributions |

### Teacher Weights

Each KD student uses the teacher trained with the **same seed** on the **same dataset**:
```
teacher_weights = experiments/{dataset}/pure/ResNet112/{seed}/best.pth
```
This means seed 0 students learn from the seed 0 teacher, etc.

### Runner Configuration

`runner.py` line 108: `runs = 1` — meaning only seed 0 was queued for each condition. Seeds 1-2 exist for some conditions (pure/logit/factor on CIFAR-10/100) because `runs` was set to 3 in an earlier version of the runner.

`runner.py` line 109: `datasets = ['Cifar100', 'Cifar10', 'SVHN', 'TinyImageNet']` — this order determines queue priority.

Queue limit: 10 concurrent PBS jobs on CHPC (`runner.py` line 8).

## Experiment Directory Structure

```
experiments/{Dataset}/{method}/{model_or_pair}/{seed}/
```

Each experiment directory contains:
| File | Contents |
|------|----------|
| `best.pth` | Best model weights only (`{'weights': model.state_dict()}`). Used for inference and as teacher weights for KD. |
| `checkpoint.pth` | Full training state: model, optimizer, scheduler, distillation module states, all metric arrays, max_acc. For resuming interrupted training. |
| `metrics.json` | Complete training curves: `train_loss[]`, `train_acc[]`, `test_loss[]`, `test_acc[]` (one entry per epoch) + `config` dict. |
| `status.json` | Quick lookup: `status` (completed/in_progress), `epoch`, `max_acc`, `config`. |
| `Accuracy.png` | Train/test accuracy curve plot (generated by `toolbox/utils.py:plot_the_things`). |
| `Loss.png` | Train/test loss curve plot. |
| `logs` | Stdout from PBS job. |
| `errors` | Stderr from PBS job (error tracebacks if failed). |

## Results — All Experiments

### CIFAR-10 (16 experiments, all completed)

| Model / Method | Seed 0 | Seed 1 | Seed 2 | Mean ± Std |
|----------------|--------|--------|--------|------------|
| Teacher (ResNet-112) | 92.54% | 94.21% | 93.89% | 93.55 ± 0.76% |
| Student pure (ResNet-56) | 93.11% | 93.29% | 93.19% | 93.20 ± 0.08% |
| Logit KD | 92.35% | 93.97% | 93.80% | 93.37 ± 0.73% |
| Factor Transfer | 93.74% | 93.57% | 93.89% | 93.73 ± 0.13% |
| Attention Transfer | 94.12% | — | — | — |
| FitNets | 93.84% | — | — | — |
| NST | 93.62% | — | — | — |
| **RKD** | **18.13%** | — | — | **CATASTROPHIC FAILURE** |

### CIFAR-100 (16 experiments, all completed)

| Model / Method | Seed 0 | Seed 1 | Seed 2 | Mean ± Std |
|----------------|--------|--------|--------|------------|
| Teacher (ResNet-112) | 71.59% | 72.88% | 72.43% | 72.30 ± 0.65% |
| Student pure (ResNet-56) | 70.43% | 71.54% | 71.54% | 71.17 ± 0.59% |
| Logit KD | 69.39% | 72.96% | 73.16% | 71.84 ± 1.73% |
| Factor Transfer | 71.20% | 70.64% | 72.13% | 71.32 ± 0.63% |
| Attention Transfer | 71.56% | — | — | — |
| FitNets | 70.23% | — | — | — |
| NST | 71.46% | — | — | — |
| **RKD** | **1.00%** | — | — | **CATASTROPHIC FAILURE** |

### TinyImageNet (3 experiments, partially complete)

| Model / Method | Seed 0 | Status |
|----------------|--------|--------|
| Teacher (ResNet-112) | 53.31% | in_progress (epoch 127/150) |
| Student pure (ResNet-56) | 55.65% | completed |
| Logit KD | 36.27% | in_progress (epoch 58/150) |

Remaining 5 methods (factor_transfer, attention_transfer, fitnets, rkd, nst) not yet queued — waiting for teacher to finish since KD needs teacher weights.

### SVHN (8 experiments, all failed)

All 8 experiments (pure ResNet-112, pure ResNet-56, + 6 KD methods, seed 0 each) failed immediately at dataset loading:

```
File "/mnt/lustre/users/iferreira/distillation-decomposition/train.py", line 109, in <module>
    Data = DATASETS[DATASET](BATCH_SIZE, seed=seed)
File "/mnt/lustre/users/iferreira/distillation-decomposition/toolbox/data_loader.py", line 126, in SVHN
    return DataHelper("SVHN", batch_size, data_root, seed)
File "/mnt/lustre/users/iferreira/distillation-decomposition/toolbox/data_loader.py", line 102, in __init__
    self.trainloader, self.testloader = get_loaders(dataset, batch_size, data_root, seed)
File "/mnt/lustre/users/iferreira/distillation-decomposition/toolbox/data_loader.py", line 59, in get_loaders
    trainset = ds(root=data_root, split='train', download=True, transform=transform_train)
File "/home/iferreira/myenv/lib/python3.12/site-packages/torchvision/datasets/svhn.py", line 75, in __init__
    import scipy.io as sio
ModuleNotFoundError: No module named 'scipy'
```

**Fix:** `pip install scipy` in CHPC environment (`/home/iferreira/myenv/`). The SVHN dataset uses `.mat` files which require scipy to load.

## Known Issues

1. **RKD catastrophic failure** — 18.13% on CIFAR-10 (near random for 10 classes), 1.00% on CIFAR-100 (literally random for 100 classes). Ran for all 150 epochs without converging. Likely cause: the RKD relational loss (distance + angle between sample pairs) has a very different scale than cross-entropy, and at `alpha=0.5` it dominates the gradient signal, preventing the student from learning discriminative features. Potential fixes: lower alpha (0.1), add loss normalisation, or check the implementation in `toolbox/distillation.py`.

2. **SVHN never ran** — scipy not installed on CHPC. All 8 SVHN experiments have empty status.json and error tracebacks.

3. **Incomplete seeds** — AT, FitNets, RKD, NST only have seed 0. Pure, logit, factor_transfer have seeds 0-2. This is because `runner.py` `runs` was changed from 3 to 1 at some point.

4. **TinyImageNet incomplete** — Teacher still training (epoch 127/150). Logit KD started but only at epoch 58/150 (uses the in-progress teacher's best.pth as it existed when queued). Remaining KD methods blocked on teacher completion.

5. **TinyImageNet student > teacher** — Pure ResNet-56 (55.65%) outperforms the ResNet-112 teacher (53.31% at epoch 127). Could change once teacher finishes, but worth watching — deeper ResNets can underperform on small images if not tuned.

6. **Small accuracy gaps** — On CIFAR-10/100, KD methods are within ~0.5–1% of pure student. See `research/accuracy_considerations.md` for discussion on why this is actually fine for the thesis.

7. **Logit KD seed variance on CIFAR-100** — Seed 0 (69.39%) is 3.77% below seed 2 (73.16%). Seed 0 is worse than pure student; seed 2 exceeds the teacher. Unusual instability.

## Archival Status

Experiment beta was **halted** on 2026-02-21. Not all runs completed:
- CIFAR-10: all completed (3 seeds for pure/logit/factor, seed 0 for AT/FitNets/NST/RKD)
- CIFAR-100: all completed (same seed structure as CIFAR-10)
- TinyImageNet: partially complete (teacher in_progress at epoch 127, logit at epoch 58)
- SVHN: all failed (scipy missing on CHPC)

Experiments archived to `analysis/experiment_beta/experiments/` on 2026-02-21.
Bug fixes and design improvements were applied before launching experiment charlie.
